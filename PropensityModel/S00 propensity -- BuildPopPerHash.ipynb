{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom geoCode.geoHash import geoHashSpark as geoHashSpark\n"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["countryCode = 'at'"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["osm = spark.table('dev_sources_osm.cleaned_total_data_set').where(f.lower(f.col('countryCode')) == countryCode).where(f.col('tags').cast('string').contains('building'))\nosm.write.mode('overwrite').saveAsTable(\"data_user_hien.\" + countryCode + \"_osm_building\")"],"metadata":{},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["buildings = spark.table('data_user_hien.' + countryCode + '_osm_building').withColumn('longitude', f.col('longitude').cast('float')).withColumn('latitude',f.col('latitude').cast('float')).where((f.col('latitude').isNotNull()) &(f.col('longitude').isNotNull()))\npop = spark.table('data_geo.geostat_population_grid_census_lat_lng')\npop = pop.withColumn('lat',f.col('lat').cast('float')).withColumn('lng',f.col('lng').cast('float')).where(f.lower(f.col('CNTR_CODE')) == countryCode).withColumn('TOT_P', f.col('TOT_P').cast('int'))\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["pop = geoHashSpark(pop,'lat','lng',5)\nbuildings =geoHashSpark(buildings,'latitude','longitude',5)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["buildings.show(n=5)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["pop = pop.where(f.col('TOT_P').isNotNull())\nbuildings = buildings.where(f.col('id').isNotNull())\nhashPop = pop.groupBy('geoHash').agg(f.sum(f.col('TOT_P')).alias('popPerHash'))\nhashBuilding = buildings.groupBy('geoHash').agg(f.count(f.col('id')).alias('buildPerHash'))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["hashPop.count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["hashData = hashPop.alias('pop').join(hashBuilding.alias('build'),hashPop.geoHash == hashBuilding.geoHash, how = 'left_outer').select(\"pop.*\", \"build.buildPerHash\")\nhashData.write.mode('overwrite').saveAsTable('data_user_hien.' + countryCode + '_geo_data')"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\nfrom pyspark.ml.feature import Bucketizer\n\ndef decile(inCol,outCol,df):\n  discretizer = QuantileDiscretizer(numBuckets=10, inputCol=inCol,outputCol=outCol)  \n  dfOut = discretizer.fit(df).transform(df)\n  return dfOut\n\ndf = decile('popPerHash','resultPop',df)\ndf = decile('buildPerHash','resultBuild',df)\n\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(df.where((f.col('resultPop') == 0)).where(f.col('resultBuild').isNotNull()).sort(f.col('resultBuild').desc()))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(df.where(f.col('resultBuild') == 0).groupBy('resultPop').agg(f.count(f.col('resultBuild')).alias('count')).sort(['count'],descending = True))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df.where(f.col('resultPop') == 0).groupBy('resultBuild').agg(f.count(f.col('resultPop')).alias('count')).sort(['count'],descending = True))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df.count()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["display(df.where(f.col('resultPop') == 9).groupBy('resultBuild').agg(f.count(f.col('resultPop')).alias('count')).sort(['count'],descending = False))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(df.where(f.col('resultBuild') == 9).groupBy('resultPop').agg(f.count(f.col('resultBuild')).alias('count')).sort(['count'],descending = False))"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"S00 propensity -- BuildPopPerHash","notebookId":1346357449862523},"nbformat":4,"nbformat_minor":0}
